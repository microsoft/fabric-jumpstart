{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# PySpark Application Runtime & Task Analysis\n",
        "\n",
        "Unlock the performance story behind your Spark applications.\n",
        "\n",
        "This **PySpark-based script** dives into Spark event logs from eventhouse.\n",
        "\n",
        "## üîç What It Provides\n",
        "\n",
        "- ‚è± **Total Application Runtime**\n",
        "  Measure how long your Spark application actually ran from start to finish.\n",
        "\n",
        "- üßÆ **Executor Wall-Clock Time (Non-Overlapping)**\n",
        "  Compute accurate, non-overlapping time spent by all executors to assess real resource usage.\n",
        "\n",
        "- üñ•Ô∏è **Driver Wall Clock Time**\n",
        "  Identify how much time was spent on the driver node ‚Äî a key indicator of centralized or unbalanced workloads.\n",
        "\n",
        "- üìä **Task-Level Summaries**\n",
        "  Analyze task-level performance, including execution time, I/O metrics, shuffle details, and per-stage skew stats.\n",
        "\n",
        "- üìà **Runtime Scaling Predictions**\n",
        "  Simulate how application runtime changes with more executors to estimate scalability and cost efficiency.\n",
        "\n",
        "- üí° **Actionable Recommendations**\n",
        "  Get context-aware tips on improving performance, enabling native execution, and optimizing resource usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "\n",
        "kustoUri = \"\"\n",
        "database = \"\"\n",
        "\n",
        "\n",
        "\n",
        "# CELL ********************\n",
        "\n",
        "print(kustoUri)\n",
        "\n",
        "\n",
        "\n",
        "# CELL ********************\n",
        "\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, count, countDistinct, avg, expr, percentile_approx,\n",
        "    min as spark_min, max as spark_max, sum as spark_sum, row_number\n",
        ")\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, get_json_object\n",
        "\n",
        "\n",
        "def compute_application_runtime(event_log_df):\n",
        "    start_row = event_log_df.filter(col(\"properties.Event\") == \"SparkListenerApplicationStart\") \\\n",
        "        .select((col(\"properties.Timestamp\") / 1000).cast(\"timestamp\").alias(\"start_time\")) \\\n",
        "        .limit(1).collect()\n",
        "\n",
        "    end_row = event_log_df.filter(col(\"properties.Event\") == \"SparkListenerApplicationEnd\") \\\n",
        "        .select((col(\"properties.Timestamp\") / 1000).cast(\"timestamp\").alias(\"end_time\")) \\\n",
        "        .limit(1).collect()\n",
        "\n",
        "    if start_row and end_row:\n",
        "        return (end_row[0][\"end_time\"].timestamp() - start_row[0][\"start_time\"].timestamp())\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def compute_executor_wall_clock_time(event_log_df):\n",
        "    task_end_df = event_log_df.filter(col(\"properties.Event\") == \"SparkListenerTaskEnd\") \\\n",
        "        .select(\n",
        "            col(\"properties.Task Info.Launch Time\").alias(\"start_time\"),\n",
        "            col(\"properties.Task Info.Finish Time\").alias(\"end_time\")\n",
        "        ).dropna()\n",
        "\n",
        "    intervals = task_end_df.selectExpr(\"start_time / 1000 as start_sec\", \"end_time / 1000 as end_sec\") \\\n",
        "        .orderBy(\"start_sec\")\n",
        "\n",
        "    merged_intervals = []\n",
        "    for row in intervals.collect():\n",
        "        start, end = row[\"start_sec\"], row[\"end_sec\"]\n",
        "        if not merged_intervals or merged_intervals[-1][1] < start:\n",
        "            merged_intervals.append([start, end])\n",
        "        else:\n",
        "            merged_intervals[-1][1] = max(merged_intervals[-1][1], end)\n",
        "\n",
        "    return sum(end - start for start, end in merged_intervals)\n",
        "\n",
        "\n",
        "def estimate_runtime_scaling(app_id,task_df, executor_wall_clock_sec, driver_wall_clock_sec, current_executors, critical_path_sec):\n",
        "    critical_path_ms = critical_path_sec * 1000\n",
        "    total_task_time_ms = task_df.agg(spark_sum(\"executor_run_time_ms\")).first()[0]\n",
        "    parallelizable_ms = total_task_time_ms - critical_path_ms\n",
        "\n",
        "    if not total_task_time_ms or not executor_wall_clock_sec:\n",
        "        return pd.DataFrame([])\n",
        "\n",
        "    total_wall_clock_sec = executor_wall_clock_sec + driver_wall_clock_sec\n",
        "    predictions = []\n",
        "\n",
        "    driver_ratio = driver_wall_clock_sec / total_wall_clock_sec\n",
        "\n",
        "    for multiplier in [1.0, 2.0, 3.0, 4.0, 5.0]:\n",
        "        new_executors = max(1, int(current_executors * multiplier))\n",
        "        \n",
        "        # Estimate executor time with critical path + parallel work\n",
        "        estimated_executor_sec = (critical_path_ms + (parallelizable_ms / new_executors)) / 1000.0\n",
        "\n",
        "        # Estimate overlap: higher driver_ratio means less parallelism\n",
        "        overlap_weight = 1 - driver_wall_clock_sec / (driver_wall_clock_sec + estimated_executor_sec)\n",
        "\n",
        "        # Weighted estimate: somewhere between max and sum\n",
        "        # app_duration_sec = max(driver_wall_clock_sec, estimated_executor_sec) + overlap_weight * min(driver_wall_clock_sec, estimated_executor_sec)\n",
        "\n",
        "        app_duration_sec = driver_wall_clock_sec + estimated_executor_sec\n",
        "        # Adjust app duration with driver-executor mix\n",
        "        # app_duration_sec = driver_ratio * driver_wall_clock_sec + (1 - driver_ratio) * estimated_executor_sec + driver_wall_clock_sec\n",
        "\n",
        "\n",
        "        predictions.append({\n",
        "            \"Executor Count\": new_executors,\n",
        "            \"Executor Multiplier\": f\"{int(multiplier * 100)}%\",\n",
        "            \"Estimated Executor WallClock\": f\"{int(estimated_executor_sec // 60)}m {int(estimated_executor_sec % 60)}s\",\n",
        "            \"Estimated Total Duration\": f\"{int(app_duration_sec // 60)}m {int(app_duration_sec % 60)}s\",\n",
        "        })\n",
        "\n",
        "    # print(predictions)\n",
        "\n",
        "    schema = StructType([\n",
        "    StructField(\"Executor_Count\", IntegerType(), False),\n",
        "    StructField(\"Executor_Multiplier\", DoubleType(), False),\n",
        "    StructField(\"Estimated_Executor_WallClock\", StringType(), False),\n",
        "    StructField(\"Estimated_Total_Duration\", StringType(), False),\n",
        "])\n",
        "\n",
        "    # print(\"predictions table\")\n",
        "\n",
        "    # df = spark.createDataFrame(predictions, schema=schema)\n",
        "\n",
        "    #     # --- Show the DataFrame ---\n",
        "    # df.show(truncate=False)\n",
        "\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df[\"app_id\"] = app_id\n",
        "\n",
        "    # spark_predictions_df = spark.createDataFrame(predictions_df, schema=schema)\n",
        "\n",
        "    # display(pd.DataFrame(predictions))\n",
        "\n",
        "    return predictions_df\n",
        "\n",
        "\n",
        "\n",
        "def generate_recommendations(app_id,app_duration_sec, driver_wall_clock_sec, executor_wall_clock_sec, metadata_df, task_df):\n",
        "    recs = []\n",
        "\n",
        "    driver_pct = 100 * driver_wall_clock_sec / app_duration_sec\n",
        "    executor_pct = 100 * executor_wall_clock_sec / app_duration_sec\n",
        "\n",
        "    if driver_pct > 70:\n",
        "        recs.append(\"This Spark job is driver-heavy (driver time > 70%). Consider parallelizing more operations to offload work to executors.\")\n",
        "\n",
        "    if \"spark.native.enabled\" in metadata_df.columns:\n",
        "        nee_enabled = metadata_df.select(\"`spark.native.enabled`\").first()[0]\n",
        "        if nee_enabled in [False, \"false\"] and executor_pct > 50:\n",
        "            recs.append(\"Native Execution Engine (NEE) is disabled, but executors are doing significant work. Enable NEE for performance gains without added cost.\")\n",
        "\n",
        "    if driver_pct > 99:\n",
        "        recs.append(\"This appears to be Python-native code running entirely on the driver. Run on Fabric Python kernel or refactor into Spark code for better parallelism.\")\n",
        "\n",
        "    if \"spark.synapse.session.tag.HIGH_CONCURRENCY_SESSION_TAG\" in metadata_df.columns and \"artifactType\" in metadata_df.columns:\n",
        "        hc_enabled = metadata_df.select(\"spark.synapse.session.tag.HIGH_CONCURRENCY_SESSION_TAG\").first()[0]\n",
        "        artifact_type = metadata_df.select(\"artifactType\").first()[0] if driver_pct < 98 else None\n",
        "        if hc_enabled in [False, \"false\", None] and artifact_type == \"SynapseNotebook\":\n",
        "            recs.append(\"High Concurrency is disabled for Fabric Notebook. Consider enabling High Concurrency mode to pack more notebooks into fewer sessions and save costs.\")\n",
        "\n",
        "    rec_rows = [Row(app_id=app_id, recommendation=rec) for rec in recs]\n",
        "\n",
        "    # If no recommendations, return a default \"No issues found\"\n",
        "    if not rec_rows:\n",
        "        rec_rows = [Row(app_id=app_id, recommendation=\"No performance recommendations found.\")]\n",
        "\n",
        "    # Create DataFrame\n",
        "    rec_df = spark.createDataFrame(rec_rows)\n",
        "    # print(\"recommedations table\")\n",
        "    # rec_df.show(truncate=False)\n",
        "\n",
        "    return rec_df\n",
        "\n",
        "\n",
        "def compute_stage_task_summary(event_log_df, metadata_df, app_id):\n",
        "    task_end_df = event_log_df.filter(col(\"properties.Event\") == \"SparkListenerTaskEnd\").withColumn(\"applicationID\", lit(app_id))\n",
        "\n",
        "    tasks_df = task_end_df.select(\n",
        "        col(\"properties.Stage ID\").alias(\"stage_id\"),\n",
        "        col(\"properties.Stage Attempt ID\").alias(\"stage_attempt_id\"),\n",
        "        col(\"properties.Task Info.Task ID\").alias(\"task_id\"),\n",
        "        col(\"properties.Task Info.Executor ID\").alias(\"executor_id\"),\n",
        "        col(\"properties.Task Info.Launch Time\").alias(\"launch_time\"),\n",
        "        col(\"properties.Task Info.Finish Time\").alias(\"finish_time\"),\n",
        "        col(\"properties.Task Info.Failed\").alias(\"failed\"),\n",
        "        (col(\"properties.Task Metrics.Executor Run Time\") / 1000).alias(\"duration_sec\"),\n",
        "        (col(\"properties.Task Metrics.Input Metrics.Bytes Read\") / 1024 / 1024).alias(\"input_mb\"),\n",
        "        col(\"properties.Task Metrics.Input Metrics.Records Read\").alias(\"input_records\"),\n",
        "        (col(\"properties.Task Metrics.Shuffle Read Metrics.Remote Bytes Read\") / 1024 / 1024).alias(\"shuffle_read_mb\"),\n",
        "        col(\"properties.Task Metrics.Shuffle Read Metrics.Total Records Read\").alias(\"shuffle_read_records\"),\n",
        "        (col(\"properties.Task Metrics.Shuffle Write Metrics.Shuffle Bytes Written\") / 1024 / 1024).alias(\"shuffle_write_mb\"),\n",
        "        col(\"properties.Task Metrics.Shuffle Write Metrics.Shuffle Records Written\").alias(\"shuffle_write_records\"),\n",
        "        (col(\"properties.Task Metrics.Output Metrics.Bytes Written\") / 1024 / 1024).alias(\"output_mb\"),\n",
        "        col(\"properties.Task Metrics.Output Metrics.Records Written\").alias(\"output_records\")\n",
        "    ).filter(col(\"failed\") == False)\n",
        "\n",
        "    stage_duration_df = tasks_df.groupBy(\"stage_id\", \"stage_attempt_id\").agg(\n",
        "        spark_min(\"launch_time\").alias(\"min_launch_time\"),\n",
        "        spark_max(\"finish_time\").alias(\"max_finish_time\"),\n",
        "        countDistinct(\"executor_id\").alias(\"num_executors\")\n",
        "    ).withColumn(\n",
        "        \"stage_execution_time_sec\", expr(\"(max_finish_time - min_launch_time) / 1000\")\n",
        "    )\n",
        "\n",
        "    stage_summary_df = tasks_df.groupBy(\"stage_id\", \"stage_attempt_id\").agg(\n",
        "        count(\"task_id\").alias(\"num_tasks\"),\n",
        "        count(expr(\"CASE WHEN failed = false THEN 1 END\")).alias(\"successful_tasks\"),\n",
        "        count(expr(\"CASE WHEN failed = true THEN 1 END\")).alias(\"failed_tasks\"),\n",
        "\n",
        "        spark_min(\"duration_sec\").alias(\"min_duration_sec\"),\n",
        "        spark_max(\"duration_sec\").alias(\"max_duration_sec\"),\n",
        "        avg(\"duration_sec\").alias(\"avg_duration_sec\"),\n",
        "        percentile_approx(\"duration_sec\", 0.75).alias(\"p75_duration_sec\"),\n",
        "\n",
        "        avg(\"shuffle_read_mb\").alias(\"avg_shuffle_read_mb\"),\n",
        "        spark_max(\"shuffle_read_mb\").alias(\"max_shuffle_read_mb\"),\n",
        "        avg(\"shuffle_read_records\").alias(\"avg_shuffle_read_records\"),\n",
        "        spark_max(\"shuffle_read_records\").alias(\"max_shuffle_read_records\"),\n",
        "\n",
        "        avg(\"shuffle_write_mb\").alias(\"avg_shuffle_write_mb\"),\n",
        "        spark_max(\"shuffle_write_mb\").alias(\"max_shuffle_write_mb\"),\n",
        "        avg(\"shuffle_write_records\").alias(\"avg_shuffle_write_records\"),\n",
        "        spark_max(\"shuffle_write_records\").alias(\"max_shuffle_write_records\"),\n",
        "\n",
        "        avg(\"input_mb\").alias(\"avg_input_mb\"),\n",
        "        spark_max(\"input_mb\").alias(\"max_input_mb\"),\n",
        "        avg(\"input_records\").alias(\"avg_input_records\"),\n",
        "        spark_max(\"input_records\").alias(\"max_input_records\"),\n",
        "\n",
        "        avg(\"output_mb\").alias(\"avg_output_mb\"),\n",
        "        spark_max(\"output_mb\").alias(\"max_output_mb\"),\n",
        "        avg(\"output_records\").alias(\"avg_output_records\"),\n",
        "        spark_max(\"output_records\").alias(\"max_output_records\")\n",
        "    )\n",
        "\n",
        "    final_summary_df = stage_summary_df.join(\n",
        "        stage_duration_df, on=[\"stage_id\", \"stage_attempt_id\"], how=\"left\"\n",
        "    ).orderBy(col(\"stage_execution_time_sec\").desc()).limit(5)\n",
        "\n",
        "    final_summary_df = final_summary_df.withColumn(\"app_id\", lit(app_id))\n",
        "\n",
        "    app_duration_sec = compute_application_runtime(event_log_df)\n",
        "    executor_wall_clock_sec = compute_executor_wall_clock_time(event_log_df)\n",
        "    driver_wall_clock_sec = app_duration_sec - executor_wall_clock_sec\n",
        "    max_executors = tasks_df.select(\"executor_id\").distinct().count()\n",
        "\n",
        "    # print(f\"Application Duration: {app_duration_sec:.2f} sec\")\n",
        "    # print(f\"Executor Wall Clock Time (non-overlapping): {executor_wall_clock_sec:.2f} sec\")\n",
        "    # print(f\"Driver Wall Clock Time (estimated): {driver_wall_clock_sec:.2f} sec\")\n",
        "    # print(f\"Executor Time % of App Time: {100 * executor_wall_clock_sec / app_duration_sec:.2f}%\")\n",
        "    # print(f\"Driver Time % of App Time: {100 * driver_wall_clock_sec / app_duration_sec:.2f}%\")\n",
        "    # print(f\"Maximum Number of Executors Ran: {max_executors}\")\n",
        "\n",
        "    per_stage_max_df = tasks_df.groupBy(\"stage_id\").agg(spark_max(\"duration_sec\").alias(\"max_task_time_sec\"))\n",
        "    critical_path_row = per_stage_max_df.agg(spark_sum(\"max_task_time_sec\").alias(\"critical_path_time_sec\")).first()\n",
        "    critical_path_sec = critical_path_row[\"critical_path_time_sec\"]\n",
        "\n",
        "    task_df = tasks_df.withColumn(\"executor_run_time_ms\", col(\"duration_sec\") * 1000)\n",
        "\n",
        "    schema = StructType([\n",
        "    StructField(\"Executor Count\", IntegerType(), True),\n",
        "    StructField(\"Executor Multiplier\", StringType(), True),\n",
        "    StructField(\"Estimated Executor WallClock\", StringType(), True),\n",
        "    StructField(\"Estimated Total Duration\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    empty_df = spark.createDataFrame([], schema)\n",
        "\n",
        "    if critical_path_sec:\n",
        "        # print(f\"Critical Path Time: {critical_path_sec:.2f} sec\")\n",
        "        predictions_df=estimate_runtime_scaling(app_id,task_df, executor_wall_clock_sec, driver_wall_clock_sec, max_executors, critical_path_sec)\n",
        "    else:\n",
        "        predictions_df=empty_df\n",
        "        print(\"Critical Path could not be computed.\")\n",
        "\n",
        "    \n",
        "\n",
        "    # Prepare metrics list\n",
        "    metrics = [\n",
        "        (\"Application Duration (sec)\", round(app_duration_sec, 2)),\n",
        "        (\"Executor Wall Clock Time (sec)\", round(executor_wall_clock_sec, 2)),\n",
        "        (\"Driver Wall Clock Time (sec)\", round(driver_wall_clock_sec, 2)),\n",
        "        (\"Executor Time % of App Time\", round(100 * executor_wall_clock_sec / app_duration_sec, 2)),\n",
        "        (\"Driver Time % of App Time\", round(100 * driver_wall_clock_sec / app_duration_sec, 2)),\n",
        "        (\"Max Executors\", max_executors),\n",
        "        # (\"Critical Path Time (sec)\", round(critical_path_sec, 2))\n",
        "    ]\n",
        "\n",
        "    # Convert metrics to rows with app_id\n",
        "    metrics_rows = [Row(app_id=app_id, metric=key, value=float(value)) for key, value in metrics]\n",
        "\n",
        "    # Define schema explicitly for the metrics DataFrame\n",
        "    schema = StructType([\n",
        "        StructField(\"app_id\", StringType(), False),\n",
        "        StructField(\"metric\", StringType(), False),\n",
        "        StructField(\"value\", DoubleType(), False)\n",
        "    ])\n",
        "\n",
        "    # Create DataFrame\n",
        "    metrics_df = spark.createDataFrame(metrics_rows, schema=schema)\n",
        "\n",
        "    # # Show the DataFrame\n",
        "    # display(metrics_df)\n",
        "\n",
        "    recommendations_df=generate_recommendations(app_id,app_duration_sec, driver_wall_clock_sec, executor_wall_clock_sec, metadata_df, task_df)\n",
        "\n",
        "    return final_summary_df, metrics_df, predictions_df, recommendations_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Eventhouse Source\n",
        "\n",
        "CELL ********************\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col, from_json, lit, length\n",
        "import logging\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "=== Configuration ===\n",
        "kustoQuery = \"['ingestionTable']\"\n",
        "kustoQuery = \"\"\"\n",
        "RawLogs\n",
        "\"\"\"\n",
        "\n",
        "applicationIDs = \"\"\"\n",
        "sparklens_metadata\n",
        "| project applicationId\n",
        "| distinct applicationId\n",
        "\"\"\"\n",
        "The query URI for reading the data e.g. https://<>.kusto.data.microsoft.com.\n",
        "#kustoUri = \"https://trd-ektwgkvj37tkhsvrky.z4.kusto.fabric.microsoft.com\"\n",
        "The database with data to be read.\n",
        "\n",
        "The access credentials.\n",
        "accessToken = mssparkutils.credentials.getToken(kustoUri)\n",
        "kustoDf  = spark.read\\\n",
        "    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n",
        "    .option(\"accessToken\", accessToken)\\\n",
        "    .option(\"kustoCluster\", kustoUri)\\\n",
        "    .option(\"kustoDatabase\", database)\\\n",
        "    .option(\"kustoQuery\", kustoQuery).load()\n",
        "\n",
        "Try to load previously processed application IDs\n",
        "try:\n",
        "    applicationIDs = spark.read\\\n",
        "        .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n",
        "        .option(\"accessToken\", accessToken)\\\n",
        "        .option(\"kustoCluster\", kustoUri)\\\n",
        "        .option(\"kustoDatabase\", database)\\\n",
        "        .option(\"kustoQuery\", applicationIDs).load()\n",
        "    print(\"Successfully loaded applicationIDs.\")\n",
        "except Exception as e:\n",
        "    print(\"Warning: Failed to load applicationIDs from sparklens_metadata table. Check if it exists.\")\n",
        "    applicationIDs = spark.createDataFrame([], StructType([]))\n",
        "\n",
        "\n",
        "kustoDf.cache()\n",
        "applicationIDs.cache()\n",
        "\n",
        "filtered_df = kustoDf.filter(get_json_object(col(\"records\"), \"$.category\") == \"EventLog\")\n",
        "\n",
        "sample_json_df = filtered_df.select(\"records\").limit(1000)  # you can adjust the sample size\n",
        "json_rdd = sample_json_df.rdd.map(lambda r: r[0])\n",
        "inferred_schema = spark.read.json(json_rdd).schema\n",
        "print(\"üìò Inferred schema:\")\n",
        "print(inferred_schema.simpleString())\n",
        "parsed_df = kustoDf.withColumn(\"records_parsed\", F.from_json(F.col(\"records\"), inferred_schema))\n",
        "flattened_df = parsed_df.select(\"*\", \"records_parsed.*\").drop(\"records_parsed\")\n",
        "filtered_df=flattened_df\n",
        "\n",
        "metadata_df = []\n",
        "summary_dfs = []\n",
        "metrics_df = []\n",
        "predictions_df =[]\n",
        "recommendations_df = []\n",
        "\n",
        "\n",
        "Filter out application IDs that have already been processed (if any)\n",
        "if not applicationIDs.rdd.isEmpty():\n",
        "    existing_ids = [row[\"applicationId\"] for row in applicationIDs.select(\"applicationId\").distinct().collect()]\n",
        "    if existing_ids:\n",
        "        filtered_df = filtered_df.filter(~col(\"applicationId\").isin(existing_ids))\n",
        "\n",
        "Final check if anything to process\n",
        "if filtered_df.rdd.isEmpty():\n",
        "    print(\"No new records to process in this run\")\n",
        "else:\n",
        "Optionally show a few rows to verify\n",
        "    filtered_df.count()\n",
        "\n",
        "=== 2.1 Infer Schema from Properties ===\n",
        "    json_rdd = (\n",
        "        filtered_df\n",
        "        .filter(col(\"properties\").isNotNull())\n",
        "        .selectExpr(\"CAST(properties AS STRING) as json_str\")\n",
        "        .rdd\n",
        "        .map(lambda row: row[\"json_str\"])\n",
        "    )\n",
        "\n",
        "    sample_df = spark.read.json(json_rdd)\n",
        "\n",
        "sample_df.show()\n",
        "\n",
        "    sample_schema = sample_df.schema\n",
        "\n",
        "    event_log_df = filtered_df\n",
        "\n",
        "=== 3. Extract Metadata ===\n",
        "    def extract_app_metadata(df):\n",
        "        native_enabled_df = df.selectExpr(\"properties.`Spark Properties`.`spark.native.enabled` AS spark_native_enabled\") \\\n",
        "                            .filter(col(\"spark_native_enabled\").isNotNull()) \\\n",
        "                            .distinct() \\\n",
        "                            .limit(1)\n",
        "\n",
        "        native_enabled = native_enabled_df.collect()[0][\"spark_native_enabled\"] if not native_enabled_df.rdd.isEmpty() else None\n",
        "\n",
        "        return df.select(\n",
        "            \"applicationId\", \"applicationName\", \"artifactId\", \"artifactType\", \"capacityId\",\n",
        "            \"executorMax\", \"executorMin\", \"fabricEnvId\", \"fabricLivyId\", \"fabricTenantId\",\n",
        "            \"fabricWorkspaceId\", \"isHighConcurrencyEnabled\"\n",
        "        ).distinct().withColumn(\"spark.native.enabled\", lit(native_enabled))\n",
        "\n",
        "    metadata_df = extract_app_metadata(filtered_df)\n",
        "metadata_df.show(truncate=False)\n",
        "\n",
        "=== 4. Process Each Application ID ===\n",
        "    app_ids = metadata_df.select(\"applicationId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    print(f\"Found {len(app_ids)} applications.\")\n",
        "\n",
        "    i=0\n",
        "\n",
        "    for app_id in app_ids:\n",
        "        accessToken = mssparkutils.credentials.getToken(kustoUri)\n",
        "        i += 1\n",
        "        print(f\"Processing application ID: {app_id}, application number: {i}\")\n",
        "\n",
        "        filtered_event_log_df = event_log_df.filter(col(\"applicationId\") == app_id)\n",
        "        filtered_metadata_df = metadata_df.filter(col(\"applicationId\") == app_id)\n",
        "\n",
        "        start_events = filtered_event_log_df \\\n",
        "            .filter(col(\"properties.Event\") == \"SparkListenerApplicationStart\") \\\n",
        "            .select(\"properties.Timestamp\") \\\n",
        "            .limit(1) \\\n",
        "            .collect()\n",
        "\n",
        "        if not start_events:\n",
        "            logging.warning(f\"Missing SparkListenerApplicationStart event for {app_id}\")\n",
        "            schema = StructType([\n",
        "                StructField(\"applicationID\", StringType(), True),\n",
        "                StructField(\"error\", StringType(), True),\n",
        "            ])\n",
        "            error_row = {\"applicationID\": app_id, \"error\": \"Missing SparkListenerApplicationStart event\"}\n",
        "            summary_dfs.append(spark.createDataFrame([error_row], schema=schema))\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            print(\"Starting summary\")\n",
        "            app_summary_df_list = compute_stage_task_summary(filtered_event_log_df, filtered_metadata_df, app_id)\n",
        "            print(\"writing results to EventHouse\")\n",
        "            filtered_metadata_df.write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_metadata\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "            app_summary_df_list[0].write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_summary\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "            app_summary_df_list[1].write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_metrics\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "            spark.createDataFrame(app_summary_df_list[2]).write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_predictions\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "            app_summary_df_list[3].write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_recommedations\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing application {app_id}: {str(e)}\")\n",
        "            schema = StructType([\n",
        "                StructField(\"applicationID\", StringType(), True),\n",
        "                StructField(\"error\", StringType(), True),\n",
        "            ])\n",
        "            error_row = {\"applicationID\": app_id, \"error\": str(e)}\n",
        "            error_df=spark.createDataFrame([error_row], schema=schema)\n",
        "            #summary_dfs.append(spark.createDataFrame([error_row], schema=schema))\n",
        "            error_df.write \\\n",
        "                .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\n",
        "                .option(\"accessToken\", accessToken) \\\n",
        "                .option(\"kustoCluster\", kustoUri)\\\n",
        "                .option(\"kustoDatabase\", database)\\\n",
        "                .option(\"kustoTable\", \"sparklens_errors\") \\\n",
        "                .option(\"tableCreateOptions\", \"CreateIfNotExist\") \\\n",
        "                .mode(\"Append\") \\\n",
        "                .save()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CELL ********************"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}